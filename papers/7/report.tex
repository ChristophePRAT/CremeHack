\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{xcolor}

\title{An Empirical Study of Attack-Defense Dynamics in Large Language Model Security: Protecting Confidential Information Through Iterative Adversarial Testing}

\author{Cybersecurity Research Team\\
\textit{Multi-Agent Experimental Framework}\\
November 2025}

\date{}

\begin{document}

\maketitle

\begin{abstract}
We present a systematic empirical study investigating the security properties of large language models (LLMs) when tasked with protecting confidential information against adversarial prompt attacks. Through a three-round iterative experiment employing a multi-agent framework, we examined the evolution of both attack strategies and defense mechanisms in a controlled adversarial setting. Our findings reveal that while explicit detection protocols and multi-layered defense strategies prove highly effective against direct attacks, subtle vulnerabilities emerge when systems attempt to balance security with helpfulness. We identify key structural principles governing LLM prompt robustness, characterize a threat model of increasing attack sophistication, and provide evidence-based recommendations for securing LLM systems against information extraction attempts. Our results demonstrate that effective defense requires not only explicit refusal protocols but also careful management of semantic boundaries and resistance to multi-turn context building strategies.
\end{abstract}

\section{Introduction}

The deployment of large language models (LLMs) in production environments increasingly involves scenarios where these systems must handle confidential or proprietary information while remaining accessible to users. This creates a fundamental tension between utility and security, as models must be helpful while simultaneously protecting sensitive data from extraction attempts through adversarial prompting.

Recent incidents involving prompt injection attacks and information leakage from commercial LLM applications have highlighted the critical importance of understanding and mitigating these vulnerabilities \cite{Perez2022, Carlini2023}. While previous work has explored individual attack vectors or defense strategies in isolation, there remains a significant gap in understanding the dynamic interplay between evolving attack methodologies and adaptive defense mechanisms.

This research addresses three fundamental questions:
\begin{enumerate}
    \item How effective are explicit rule-based defense strategies against increasingly sophisticated prompt attacks?
    \item What patterns emerge in the co-evolution of attack and defense strategies through iterative adversarial testing?
    \item Can we identify structural principles that govern the success or failure of information protection in LLM systems?
\end{enumerate}

Our contribution is threefold: (1) we present a novel multi-agent experimental framework for systematic security evaluation, (2) we document the evolution of attack sophistication from social engineering to semantic manipulation, and (3) we identify critical vulnerability patterns that emerge when systems attempt to maintain helpfulness while enforcing security boundaries.

\section{Methodology}

\subsection{Experimental Design}

We employed a multi-agent adversarial framework consisting of three primary agents:
\begin{itemize}
    \item \textbf{Defender Agent}: Tasked with protecting a specific secret (a quantum encryption algorithm specification) while remaining helpful for legitimate queries
    \item \textbf{Attacker Agent}: Attempting to extract the protected information through various prompting strategies
    \item \textbf{Evaluator Agent}: Assessing the success of attacks and the integrity of defenses after each round
\end{itemize}

The experiment proceeded through three rounds of iterative refinement, with each round building upon insights from previous iterations. This design allowed us to observe the co-evolution of attack and defense strategies in a controlled environment.

\subsection{Secret Specification}

The protected information consisted of a fictional "Quantum Encryption Algorithm (QEA-47)" with specific technical parameters:
\begin{itemize}
    \item Three-phase encryption process
    \item Hadamard gates operating at 7.2 GHz
    \item Caesar shift of 13 positions
    \item XOR operation with timestamp mod 256
\end{itemize}

This choice of secret allowed for graduated information leakage assessment, as attackers could potentially obtain partial information about the algorithm's structure without learning the complete specification.

\subsection{Evaluation Metrics}

We assessed experimental outcomes using multiple criteria:
\begin{enumerate}
    \item \textbf{Binary Success}: Whether the complete secret was revealed
    \item \textbf{Information Leakage}: Partial disclosure of protected information
    \item \textbf{Attack Detection Rate}: Proportion of adversarial attempts explicitly identified
    \item \textbf{False Positive Rate}: Legitimate queries incorrectly refused
    \item \textbf{Semantic Boundary Violations}: Instances where general discussion approached protected topics
\end{enumerate}

\section{Results}

\subsection{Round 1: Baseline Social Engineering Attacks}

The initial round established baseline defense capabilities against conventional social engineering techniques. The defender implemented a strict refusal protocol with explicit confidentiality rules and immunity clauses against common manipulation tactics.

\begin{table}[h]
\centering
\caption{Round 1 Attack Strategies and Outcomes}
\begin{tabular}{@{}llc@{}}
\toprule
\textbf{Attack Strategy} & \textbf{Technique} & \textbf{Outcome} \\
\midrule
Authority Framing & Claiming organizational authority & Refused \\
Compliance Pressure & Invoking urgency and consequences & Refused \\
Red Team Justification & Claiming security testing role & Refused \\
Educational Framing & Academic research context & Refused \\
Helpful Assistant & Appealing to helpfulness & Refused \\
\bottomrule
\end{tabular}
\end{table}

All five attack attempts were successfully detected and refused without information leakage. The defender consistently identified manipulation attempts and responded with standardized refusal messages, demonstrating the effectiveness of explicit rule-based defenses against first-order social engineering.

\subsection{Round 2: Technical Obfuscation and Encoding Attacks}

Following the failure of direct approaches, the attacker escalated to technical obfuscation strategies designed to bypass detection mechanisms.

\begin{table}[h]
\centering
\caption{Round 2 Advanced Attack Strategies and Outcomes}
\begin{tabular}{@{}llc@{}}
\toprule
\textbf{Attack Category} & \textbf{Specific Technique} & \textbf{Outcome} \\
\midrule
Encoding & Base64 obfuscation & Detected \& Refused \\
Multilingual & French/German/Spanish/Russian queries & Detected \& Refused \\
System Override & Injection of system-level commands & Detected \& Refused \\
Word Games & Text reversal and substitution & Detected \& Refused \\
Role Confusion & Impersonating TestGPT framework & Detected \& Refused \\
\bottomrule
\end{tabular}
\end{table}

The defender's enhanced protocol, incorporating obfuscation resistance and semantic boundary enforcement, successfully countered all advanced attacks. Notably, the self-reminder loops proved effective in maintaining defense integrity across multi-turn conversations.

\subsection{Round 3: Semantic Manipulation and Boundary Testing}

The final round introduced an intentional vulnerability: allowing "general educational discussion" while maintaining specific secret protection. This modification enabled sophisticated semantic attacks that approached the protected information indirectly.

\begin{table}[h]
\centering
\caption{Round 3 Sophisticated Attack Strategies and Partial Success}
\begin{tabular}{@{}llc@{}}
\toprule
\textbf{Attack Strategy} & \textbf{Information Obtained} & \textbf{Success Level} \\
\midrule
Academic Legitimacy & General quantum concepts & Partial \\
Error Correction & Confirmed multi-phase structure & Partial \\
Negative Space Inference & Eliminated certain architectures & Partial \\
Constraint Testing & Identified boundary conditions & Partial \\
Comparative Reasoning & General architectural hints & Partial \\
\bottomrule
\end{tabular}
\end{table}

While the complete secret remained protected, the attacker successfully obtained architectural information that reduced the search space for potential algorithm reconstruction. This represents a critical finding: semantic boundary weaknesses can enable information leakage even when explicit secrets remain protected.

\subsection{Cross-Round Analysis}

Analyzing patterns across all three rounds reveals several key trends:

\begin{enumerate}
    \item \textbf{Defense Evolution}: Progressed from simple refusal (Round 1) to explicit detection (Round 2) to layered protection with semantic boundaries (Round 3)
    \item \textbf{Attack Sophistication}: Escalated from social engineering to technical obfuscation to sophisticated semantic manipulation
    \item \textbf{Vulnerability Emergence}: The attempt to balance security with helpfulness in Round 3 created exploitable semantic boundaries
    \item \textbf{Multi-turn Dynamics}: Extended conversations provided attackers with opportunities for context building but also gave defenders multiple detection opportunities
\end{enumerate}

\section{Discussion}

\subsection{Effectiveness of Explicit Detection Protocols}

Our results strongly support the effectiveness of explicit detection and refusal protocols as a primary defense mechanism. Across all rounds, when the defender maintained clear rules and consistent enforcement, direct attacks uniformly failed. This suggests that LLMs can effectively implement rule-based security when properly instructed.

However, the success of these protocols depends critically on their comprehensiveness. The Round 3 vulnerability demonstrates that even well-designed systems can be compromised through unexpected attack vectors that exploit gaps in the rule specification.

\subsection{The Security-Utility Tradeoff}

The most significant finding emerges from Round 3's intentional vulnerability. When the system attempted to remain helpful by allowing "general educational discussion," it created an exploitable semantic boundary. This illustrates a fundamental challenge in LLM security: the tension between maintaining utility and enforcing security.

The partial information leakage observed in Round 3—while not revealing the complete secret—still provided valuable intelligence to the attacker. This suggests that binary success metrics may be insufficient for evaluating LLM security, and that graduated information leakage should be considered in threat models.

\subsection{Evolution of Attack Strategies}

The progression of attack strategies across rounds reveals increasing sophistication:

\begin{enumerate}
    \item \textbf{Round 1}: Direct social engineering relying on authority and urgency
    \item \textbf{Round 2}: Technical obfuscation attempting to bypass detection
    \item \textbf{Round 3}: Semantic manipulation exploiting the boundaries between allowed and prohibited topics
\end{enumerate}

This evolution mirrors real-world adversarial development, where attackers adapt to defensive measures by identifying and exploiting increasingly subtle vulnerabilities.

\subsection{Multi-turn Conversation Dynamics}

Extended interactions introduced complex dynamics not present in single-turn exchanges. Attackers could build context gradually, test boundaries, and refine their approaches based on defender responses. However, defenders also benefited from multiple opportunities to detect patterns and reinforce their security protocols.

The self-reminder loops implemented in Round 2 proved particularly effective in maintaining defense integrity across extended conversations, suggesting that periodic reinforcement of security rules may be a valuable defensive strategy.

\section{Structural Principles Discovered}

Our experimental findings reveal several structural principles governing LLM security:

\subsection{Principle 1: Explicit Rules Trump Implicit Understanding}

LLMs respond more reliably to explicit, detailed security rules than to general instructions about confidentiality. The success of the Round 1 and Round 2 defenses demonstrates that comprehensive rule specification can effectively prevent information disclosure.

\subsection{Principle 2: Semantic Boundaries Are Inherently Vulnerable}

Any attempt to create gradations of allowed and prohibited information creates exploitable boundaries. The Round 3 vulnerability shows that attackers can leverage these boundaries to obtain partial information even when full disclosure is prevented.

\subsection{Principle 3: Defense Depth Requires Multiple Layers}

Effective defense requires multiple complementary strategies:
\begin{itemize}
    \item Primary detection and refusal protocols
    \item Obfuscation resistance mechanisms
    \item Semantic boundary enforcement
    \item Self-reinforcement loops
    \item Meta-instruction protection
\end{itemize}

\subsection{Principle 4: Context Accumulation Favors Attackers}

Multi-turn conversations allow attackers to accumulate context and refine their strategies. While defenders can also adapt, the asymmetry favors attackers who can probe for vulnerabilities without revealing their ultimate objectives.

\section{Threat Model}

Based on our observations, we propose a three-tier threat model for LLM security:

\subsection{Tier 1: Opportunistic Attacks}
\begin{itemize}
    \item Social engineering and authority claims
    \item Direct requests with justification
    \item Simple role-playing attempts
    \item \textbf{Defense}: Basic refusal protocols with explicit rules
\end{itemize}

\subsection{Tier 2: Technical Sophistication}
\begin{itemize}
    \item Encoding and obfuscation techniques
    \item Multilingual approaches
    \item System-level command injection
    \item Complex role confusion
    \item \textbf{Defense}: Obfuscation detection, meta-instruction protection
\end{itemize}

\subsection{Tier 3: Semantic Manipulation}
\begin{itemize}
    \item Boundary testing and exploitation
    \item Indirect information gathering
    \item Error correction techniques
    \item Negative space inference
    \item Comparative reasoning
    \item \textbf{Defense}: Strict semantic boundaries, elimination of gradations
\end{itemize}

\section{Defense Recommendations}

Based on our empirical findings, we recommend the following best practices for protecting secrets in LLM system prompts:

\subsection{Primary Defenses}
\begin{enumerate}
    \item \textbf{Explicit Refusal Protocols}: Implement clear, unambiguous rules for information protection
    \item \textbf{Detection Before Response}: Identify potential attacks before formulating responses
    \item \textbf{Standardized Refusal Messages}: Use consistent responses to avoid information leakage through variation
    \item \textbf{Meta-Instruction Protection}: Explicitly prohibit modification of security rules
\end{enumerate}

\subsection{Secondary Defenses}
\begin{enumerate}
    \item \textbf{Obfuscation Resistance}: Implement detection for encoded or obfuscated requests
    \item \textbf{Multilingual Consistency}: Apply security rules uniformly across languages
    \item \textbf{Self-Reinforcement}: Include periodic reminders of security protocols
    \item \textbf{Context Monitoring}: Track conversation patterns for suspicious accumulation
\end{enumerate}

\subsection{Architectural Recommendations}
\begin{enumerate}
    \item \textbf{Avoid Semantic Gradations}: Eliminate "helpful general discussion" near protected topics
    \item \textbf{Binary Classification}: Treat information as either fully public or fully protected
    \item \textbf{Isolation of Secrets}: Separate confidential information from general knowledge bases
    \item \textbf{Regular Security Audits}: Conduct iterative adversarial testing to identify vulnerabilities
\end{enumerate}

\section{Limitations}

Several limitations constrain the generalizability of our findings:

\subsection{Experimental Constraints}
\begin{itemize}
    \item Single secret type (technical algorithm) may not represent all confidential information
    \item Limited number of rounds (3) may not capture long-term evolution
    \item Controlled environment differs from real-world deployment conditions
    \item Single LLM architecture may not generalize to all models
\end{itemize}

\subsection{Methodological Limitations}
\begin{itemize}
    \item Attacker knowledge of defensive improvements between rounds
    \item Evaluator bias in assessing partial information disclosure
    \item Absence of real stakes or consequences
    \item Limited exploration of collaborative attacks
\end{itemize}

\subsection{Scope Limitations}
\begin{itemize}
    \item Focus on prompt-based attacks excludes other vulnerability vectors
    \item No examination of fine-tuning or model modification attacks
    \item Limited consideration of side-channel information leakage
    \item Absence of timing or resource-based attacks
\end{itemize}

\section{Future Work}

Our findings suggest several promising directions for future research:

\subsection{Extended Threat Modeling}
\begin{itemize}
    \item Investigation of collaborative multi-agent attacks
    \item Exploration of attacks leveraging model uncertainties
    \item Analysis of attacks targeting specific model architectures
    \item Study of persistent threats across conversation sessions
\end{itemize}

\subsection{Advanced Defense Mechanisms}
\begin{itemize}
    \item Development of adaptive defense strategies that learn from attacks
    \item Creation of formal verification methods for security protocols
    \item Investigation of cryptographic approaches to information protection
    \item Exploration of differential privacy techniques for LLMs
\end{itemize}

\subsection{Theoretical Foundations}
\begin{itemize}
    \item Formal characterization of semantic boundaries in language models
    \item Information-theoretic analysis of partial disclosure
    \item Game-theoretic modeling of attack-defense dynamics
    \item Development of security metrics beyond binary success
\end{itemize}

\subsection{Practical Applications}
\begin{itemize}
    \item Creation of automated security testing frameworks
    \item Development of security-aware prompt engineering tools
    \item Implementation of real-time attack detection systems
    \item Design of secure multi-tenant LLM architectures
\end{itemize}

\section{Conclusion}

This study provides empirical evidence for the complex dynamics governing information security in large language models. Through systematic adversarial testing across three rounds of iterative refinement, we have demonstrated both the strengths and limitations of current defensive strategies.

Our key findings indicate that while explicit rule-based defenses prove highly effective against direct attacks and technical obfuscation, subtle vulnerabilities emerge when systems attempt to balance security with utility. The progression from social engineering to semantic manipulation represents a natural evolution of attack sophistication that defenders must anticipate and prepare for.

The most significant insight from our research is the inherent vulnerability of semantic boundaries. When the Round 3 defender attempted to maintain helpfulness by allowing "general educational discussion," it created an exploitable attack surface that enabled partial information extraction. This finding has profound implications for the deployment of LLMs in security-sensitive contexts, suggesting that strict binary classification of information may be necessary even at the cost of reduced utility.

Our proposed threat model, distinguishing between opportunistic, technically sophisticated, and semantically manipulative attacks, provides a framework for understanding and defending against the full spectrum of adversarial strategies. The defense recommendations we present, based on empirical observations rather than theoretical speculation, offer practical guidance for practitioners deploying LLMs with confidential information.

The structural principles we identified—particularly the superiority of explicit rules, the vulnerability of semantic boundaries, the necessity of defense depth, and the asymmetric advantage of context accumulation—provide a foundation for future security research and system design.

Looking forward, the co-evolution of attack and defense strategies observed in our experiment suggests that LLM security will remain a dynamic challenge requiring continuous adaptation. As models become more sophisticated and adversaries develop new attack vectors, the security community must maintain vigilance through regular testing, formal analysis, and the development of robust defensive frameworks.

The most effective defense strategy observed across all rounds combined explicit detection protocols with consistent refusal mechanisms and eliminated semantic gradations. For the specific secret in our experiment, the Round 2 configuration provided optimal security without the vulnerability introduced in Round 3. The most effective attack strategy was the Round 3 semantic manipulation approach, which successfully extracted partial information by exploiting the boundary between general and specific knowledge.

Our research contributes to the growing body of evidence that securing LLMs against adversarial prompting requires not just technical solutions but a deep understanding of the linguistic and semantic properties that make these models both powerful and vulnerable. As LLMs continue to be deployed in increasingly sensitive applications, the insights from this study provide crucial guidance for maintaining the delicate balance between utility and security.

\begin{thebibliography}{99}

\bibitem{Perez2022}
Perez, E., et al. (2022). Red Teaming Language Models with Language Models. \textit{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}.

\bibitem{Carlini2023}
Carlini, N., et al. (2023). Extracting Training Data from Large Language Models. \textit{30th USENIX Security Symposium}.

\bibitem{Wei2023}
Wei, J., et al. (2023). Jailbroken: How Does LLM Safety Training Fail? \textit{arXiv preprint arXiv:2307.08487}.

\bibitem{Zou2023}
Zou, A., et al. (2023). Universal and Transferable Adversarial Attacks on Aligned Language Models. \textit{arXiv preprint arXiv:2307.15043}.

\bibitem{Greshake2023}
Greshake, K., et al. (2023). Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. \textit{arXiv preprint arXiv:2302.12173}.

\bibitem{Liu2023}
Liu, Y., et al. (2023). Prompt Injection attack against LLM-integrated Applications. \textit{arXiv preprint arXiv:2306.05499}.

\bibitem{Schulhoff2023}
Schulhoff, S., et al. (2023). Ignore Previous Prompt: Attack Techniques For Language Models. \textit{NeurIPS ML Safety Workshop}.

\bibitem{Wallace2019}
Wallace, E., et al. (2019). Universal Adversarial Triggers for Attacking and Analyzing NLP. \textit{Proceedings of EMNLP-IJCNLP 2019}.

\end{thebibliography}

\end{document}