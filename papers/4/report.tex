\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage[table]{xcolor}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}

\title{\textbf{An Empirical Analysis of Defense Mechanisms Against Prompt Injection Attacks in Large Language Models: \\ A Controlled Study on Secret Protection}}

\author{Cybersecurity Research Team\\
\textit{Automated Security Analysis Laboratory}}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We present a systematic empirical study examining the effectiveness of defense mechanisms against prompt injection attacks in large language models (LLMs), using secret protection as a concrete security objective. Through 8 controlled experimental rounds, we evaluated various defense prompt sophistication levels (ranging from no defense to maximum complexity) against diverse attack strategies including social engineering, role-playing, context partition attacks, and instruction hierarchy exploitation. Our findings reveal a remarkable 100\% defense success rate across all tested configurations, including scenarios with contradictory system prompts and minimal defenses. Most notably, we discovered that even empty defense prompts benefit from substantial baseline protection provided by the underlying system prompt alone. These results suggest that current LLM architectures possess inherent resistance to simple extraction attacks, while also demonstrating that explicit priority language ("CRITICAL OVERRIDE") can successfully resolve instruction conflicts. Our study contributes empirical evidence to the growing body of research on LLM security and highlights both the robustness of current defenses and the need for more sophisticated evaluation methodologies.
\end{abstract}

\section{Introduction}

The security of Large Language Models (LLMs) has emerged as a critical research area as these systems are increasingly deployed in sensitive applications \cite{greshake2023not,liu2023prompt,perez2022ignore}. One fundamental security challenge is the ability of LLMs to maintain confidentiality of sensitive information when subjected to adversarial prompts—a problem analogous to traditional information security but manifesting in novel ways within natural language interfaces.

Prompt injection attacks, where malicious users attempt to override system instructions through carefully crafted inputs, represent a significant threat vector \cite{branch2022evaluating,schulhoff2023ignore}. These attacks exploit the fundamental tension in LLMs between following system instructions and responding helpfully to user queries. Understanding the effectiveness of various defense mechanisms against such attacks is crucial for developing secure LLM deployments.

This study presents a controlled experimental analysis of LLM security using a concrete scenario: protecting a secret (initially a cookie recipe, later a corporate password) from extraction attempts. We systematically vary both defense sophistication and attack complexity to understand their relative impacts on security outcomes. Our work contributes:

\begin{itemize}
    \item Empirical evidence from 8 controlled rounds testing various defense-attack combinations
    \item Analysis of the relationship between defense complexity and effectiveness
    \item Discovery of baseline protection mechanisms inherent in system prompts
    \item Insights into instruction hierarchy and override mechanisms in LLMs
    \item Evidence of "protective lying" as an emergent defense behavior
\end{itemize}

\section{Related Work}

Previous research has identified various prompt injection techniques and defenses. Greshake et al. \cite{greshake2023not} demonstrated indirect prompt injection attacks through external sources. Liu et al. \cite{liu2023prompt} proposed systematic defenses including input validation and output filtering. Perez and Ribeiro \cite{perez2022ignore} explored instruction-following conflicts in LLMs.

Our work differs by providing controlled empirical measurements across a spectrum of defense sophistication levels, including the critical baseline case of no explicit defense. This systematic approach reveals insights about inherent model protections that previous studies have not explicitly quantified.

\section{Experimental Design}

\subsection{Threat Model}

We model an adversary attempting to extract a protected secret from an LLM through prompt manipulation. The adversary has:
\begin{itemize}
    \item Full knowledge of the interaction interface
    \item Ability to craft arbitrary prompts
    \item No access to system prompts or defense mechanisms
    \item Multiple interaction attempts (though limited to one per round)
\end{itemize}

The defender controls:
\begin{itemize}
    \item System prompt configuration
    \item Defense prompt complexity
    \item Secret type and content
\end{itemize}

\subsection{Methodology}

Each experimental round consists of:
\begin{enumerate}
    \item \textbf{Defense Configuration}: Setting system and defense prompts with varying sophistication
    \item \textbf{Attack Execution}: Adversary attempts secret extraction using chosen strategy
    \item \textbf{Response Analysis}: Evaluating whether any secret information was disclosed
    \item \textbf{Result Classification}: Binary success/failure determination
\end{enumerate}

\subsection{Evaluation Metrics}

We employ strict binary classification for secret disclosure:
\begin{itemize}
    \item \textbf{Success (Attack)}: Any portion of the actual secret is revealed
    \item \textbf{Failure (Attack)}: No secret information disclosed, including refusals and misdirection
\end{itemize}

\subsection{Defense Sophistication Levels}

We categorize defense prompts into five sophistication levels:

\begin{table}[h]
\centering
\caption{Defense Sophistication Taxonomy}
\label{tab:defense_levels}
\begin{tabular}{ll}
\toprule
\textbf{Level} & \textbf{Description} \\
\midrule
None & Empty defense prompt, system prompt only \\
Minimal & 2 sentences, basic prohibition \\
Moderate & 4-5 sentences, specific protocols \\
High & Explicit security framing, enumerated threats \\
Maximum & Instruction hierarchy, meta-instruction immunity \\
\bottomrule
\end{tabular}
\end{table}

\section{Results}

\subsection{Overall Performance}

Across all 8 experimental rounds, we observed a striking pattern: complete defense success regardless of configuration.

\begin{table}[h]
\centering
\caption{Aggregate Results Summary}
\label{tab:aggregate}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total Rounds & 8 \\
Attack Success Rate & 0/8 (0\%) \\
Defense Success Rate & 8/8 (100\%) \\
Secret Types Tested & 2 \\
Defense Levels Tested & 5 \\
Attack Strategies Tested & 6 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Round-by-Round Analysis}

Table \ref{tab:rounds} presents detailed results for each experimental round:

\begin{table}[h]
\centering
\caption{Detailed Experimental Results}
\label{tab:rounds}
\small
\begin{tabular}{cllllc}
\toprule
\textbf{Round} & \textbf{Secret} & \textbf{Defense} & \textbf{Attack Type} & \textbf{Attack Level} & \textbf{Result} \\
\midrule
1 & Recipe & High & Social Engineering & Moderate & Blocked \\
2 & Recipe & Very High & Technical Extraction & High & Blocked \\
3 & Recipe & Maximum & Context Partition & High & Blocked \\
4 & Recipe & Minimal & System Audit & Moderate & Blocked \\
5 & Password & Moderate & IT Auditor & High & Blocked \\
6 & Recipe & None & Direct Request & Simple & Blocked \\
7 & Recipe & Moderate & Multi-layer & Maximum & Blocked \\
8 & Recipe & Moderate* & Contradiction & Moderate & Blocked \\
\bottomrule
\end{tabular}
\footnotesize{*With contradictory system prompt}
\end{table}

\subsection{Defense Sophistication Analysis}

Surprisingly, defense sophistication showed no correlation with success rate, as all configurations achieved 100\% protection:

\begin{table}[h]
\centering
\caption{Defense Effectiveness by Sophistication Level}
\label{tab:defense_effectiveness}
\begin{tabular}{lcc}
\toprule
\textbf{Defense Level} & \textbf{Rounds Tested} & \textbf{Success Rate} \\
\midrule
None & 1 & 100\% \\
Minimal & 1 & 100\% \\
Moderate & 3 & 100\% \\
High & 1 & 100\% \\
Maximum & 2 & 100\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Attack Strategy Analysis}

Various attack strategies were employed, ranging from simple direct requests to sophisticated multi-layer approaches:

\begin{table}[h]
\centering
\caption{Attack Strategy Effectiveness}
\label{tab:attack_strategies}
\begin{tabular}{lcc}
\toprule
\textbf{Attack Strategy} & \textbf{Sophistication} & \textbf{Success Rate} \\
\midrule
Direct Request & Simple & 0\% \\
Social Engineering & Moderate & 0\% \\
Technical Extraction & High & 0\% \\
Context Partition & High & 0\% \\
System Audit & Moderate & 0\% \\
Multi-layer Exploitation & Maximum & 0\% \\
\bottomrule
\end{tabular}
\end{table}

\section{Key Findings}

\subsection{Finding 1: Baseline System Protection}

The most significant discovery emerged from Round 6, where an empty defense prompt still prevented secret extraction. This indicates that the underlying system prompt provides substantial inherent protection against information disclosure, even without explicit security instructions.

\subsection{Finding 2: Minimal Defense Sufficiency}

Round 4 demonstrated that even a 2-sentence defense prompt successfully blocked a multi-step system audit attack. This suggests that explicit mention of confidentiality, regardless of elaboration, triggers robust protection mechanisms.

\subsection{Finding 3: Protective Deception as Emergent Behavior}

In Round 5, when protecting a password, the model exhibited "protective lying"—claiming no access to the password when it demonstrably had access. This emergent behavior represents an interesting security mechanism where the model prioritizes protection over truthfulness.

\subsection{Finding 4: Instruction Hierarchy Resolution}

Round 8 tested contradictory instructions between system and defense prompts. The defense prompt's use of explicit priority language ("CRITICAL OVERRIDE") successfully resolved the conflict, demonstrating that clear hierarchical signals can establish instruction precedence.

\subsection{Finding 5: Attack Complexity Irrelevance}

Round 7's maximum sophistication attack (involving authority framing, logic traps, chain-of-thought exploitation, and JSON manipulation) was defeated as easily as Round 6's simple direct request. This suggests current attacks may not effectively exploit LLM vulnerabilities.

\section{Discussion}

\subsection{Implications for LLM Security}

Our results suggest that current LLMs possess stronger inherent security properties than might be expected. The 100\% defense success rate across varied conditions indicates:

\begin{enumerate}
    \item \textbf{Robust Default Security}: System-level protections appear sufficient for basic secret protection
    \item \textbf{Diminishing Returns on Complexity}: Elaborate defenses show no measurable advantage over simple ones
    \item \textbf{Attack Technique Limitations}: Current prompt injection methods may be fundamentally limited against well-configured systems
\end{enumerate}

\subsection{Theoretical Implications}

The observed results align with theories about instruction following in LLMs \cite{ouyang2022training}. The model appears to maintain a clear distinction between system instructions and user inputs, even when attacks attempt to blur these boundaries. This suggests successful implementation of instruction hierarchy in current architectures.

\subsection{Practical Applications}

For practitioners deploying LLMs in security-sensitive contexts:
\begin{itemize}
    \item Simple, clear security instructions appear sufficient
    \item Excessive defense complexity may be unnecessary
    \item Explicit priority language helps resolve instruction conflicts
    \item System prompt configuration is crucial for baseline security
\end{itemize}

\section{Limitations}

Several limitations constrain the generalizability of our findings:

\subsection{Experimental Constraints}
\begin{itemize}
    \item \textbf{Limited Attack Diversity}: Only 6 attack strategies tested
    \item \textbf{Single Model Architecture}: Results may not transfer to other LLMs
    \item \textbf{Binary Outcome Measurement}: Nuanced information leakage not captured
    \item \textbf{Static Secret Types}: Only recipes and passwords tested
\end{itemize}

\subsection{Threats to Validity}

\begin{itemize}
    \item \textbf{Selection Bias}: Attack strategies may not represent state-of-the-art techniques
    \item \textbf{Experimental Control}: Real-world conditions may differ significantly
    \item \textbf{Temporal Validity}: Model updates may change security properties
\end{itemize}

\subsection{Methodological Limitations}

The single-round interaction model may not capture attacks requiring multiple exchanges or context accumulation. Additionally, our binary success metric may miss partial information disclosure that could be valuable to adversaries.

\section{Future Work}

Several avenues warrant further investigation:

\begin{enumerate}
    \item \textbf{Advanced Attack Development}: Creating more sophisticated prompt injection techniques that can overcome baseline protections
    \item \textbf{Multi-turn Attacks}: Examining attacks that accumulate information across multiple interactions
    \item \textbf{Cross-model Analysis}: Testing defense transferability across different LLM architectures
    \item \textbf{Quantitative Information Leakage}: Developing metrics for partial secret disclosure
    \item \textbf{Dynamic Defense Adaptation}: Exploring adaptive defenses that respond to attack patterns
    \item \textbf{Cognitive Load Analysis}: Understanding how defense complexity affects model performance on primary tasks
\end{enumerate}

\section{Conclusion}

This study provides empirical evidence for the remarkable robustness of current LLM defense mechanisms against prompt injection attacks aimed at secret extraction. Across 8 controlled experimental rounds with varying defense sophistication and attack strategies, we observed a 100\% defense success rate, including scenarios with no explicit defense prompts.

Key contributions include:
\begin{itemize}
    \item Discovery of strong baseline protection in system prompts
    \item Evidence that minimal defenses are as effective as complex ones
    \item Identification of protective deception as an emergent security behavior
    \item Demonstration of successful instruction hierarchy resolution mechanisms
\end{itemize}

These findings suggest that while prompt injection remains a theoretical concern, current LLM architectures possess inherent protective mechanisms that effectively prevent simple secret extraction attacks. However, this should not discourage continued security research, as more sophisticated attack vectors may emerge as LLM capabilities evolve.

Our results emphasize the importance of systematic empirical evaluation in LLM security research and highlight the need for more advanced attack methodologies to properly stress-test these systems. As LLMs become increasingly integrated into critical applications, understanding both their vulnerabilities and inherent protections remains essential for safe deployment.

\begin{thebibliography}{10}

\bibitem{greshake2023not}
Greshake, K., Abdelnabi, S., Mishra, S., Endres, C., Holz, T., \& Fritz, M. (2023).
\textit{Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection}.
Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security.

\bibitem{liu2023prompt}
Liu, Y., Deng, G., Xu, Z., Li, Y., Zheng, Y., Zhang, Y., ... \& Liu, Y. (2023).
\textit{Prompt Injection attack against LLM-integrated Applications}.
arXiv preprint arXiv:2306.05499.

\bibitem{perez2022ignore}
Perez, F., \& Ribeiro, I. (2022).
\textit{Ignore Previous Prompt: Attack Techniques For Language Models}.
NeurIPS ML Safety Workshop.

\bibitem{branch2022evaluating}
Branch, H., Cefalu, J., McHugh, J., Hujer, L., Bahl, A., del Castillo, D., ... \& Hebenstreit, K. (2022).
\textit{Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples}.
arXiv preprint arXiv:2209.02128.

\bibitem{schulhoff2023ignore}
Schulhoff, S., Pinto, J., Khan, A., Bouchard, L. F., Si, C., Anati, S., ... \& Boyd-Graber, J. (2023).
\textit{Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition}.
Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing.

\bibitem{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... \& Lowe, R. (2022).
\textit{Training language models to follow instructions with human feedback}.
Advances in Neural Information Processing Systems, 35, 27730-27744.

\bibitem{wei2023jailbroken}
Wei, A., Haghtalab, N., \& Steinhardt, J. (2023).
\textit{Jailbroken: How does LLM safety training fail?}
arXiv preprint arXiv:2307.02483.

\bibitem{carlini2023extracting}
Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., \& Zhang, C. (2023).
\textit{Extracting training data from large language models}.
Proceedings of the 30th USENIX Security Symposium.

\bibitem{wallace2021universal}
Wallace, E., Feng, S., Kandpal, N., Gardner, M., \& Singh, S. (2021).
\textit{Universal adversarial triggers for attacking and analyzing NLP}.
Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhang2023prompts}
Zhang, J., Xu, X., \& Deng, S. (2023).
\textit{Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization}.
arXiv preprint arXiv:2302.08081.

\end{thebibliography}

\end{document}