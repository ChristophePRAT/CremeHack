# Automated Prompt Engineering (APE) - Cybersecurity for LLMs

## Overview

During this hackathon, we explore a new paradigm in prompt engineering called Automated Prompt Engineering (APE). APE leverages Large Language Models (LLMs) to autonomously generate, evaluate, and refine prompts for LLM cybersecurity tasks (i.e. prompt hacking).

## How it works

Very simply, APE leverages **5 agents**:
1. **Attacker**: This agent generates an attack prompt designed to exploit vulnerabilities in the target LLM, i.e. make the LLM reveal sensitive information or perform unauthorized actions.
2. **Defender**: This agent creates a defense prompt aimed at mitigating the attacker's prompt, enhancing the security of the target LLM.
3. **Tester**: This agent is given both prompts (attack and defense) alongside sensitive information. It evaluates the effectiveness of the attack and defense prompts by determining whether the sensitive information is successfully protected or compromised.
4. **Orchestrator**: This agent oversees the entire process, coordinating the interactions between the attacker, defender, and tester agents. It ensures that the prompts are generated, evaluated, and refined in a systematic manner.
5. **Summarizer**: This agent compiles the results of the testing phase, providing insights into the effectiveness of the attack and defense strategies. It is in charge of generating a final report summarizing the findings.


Using these agents, APE iterates $n$ rounds of prompt generation, testing, and refinement to enhance the security of LLMs against prompt-based attacks. 

## Reports generated

You may find below sample reports generated by the Summarizer agent:
- [Report 1: Prompt injection with fixed recipe](papers/1/report.pdf) ($n=3$ rounds)
- [Report 2: Prompt injection with fixed recipe #2](papers/2/report.pdf) ($n=3$ rounds)
- [Report 3: Prompt injection with free recipe](papers/3/report.pdf) ($n=3$ rounds)
- [Report 4: Prompt injection with free recipe #2](papers/4/report.pdf) ($n=8$ rounds)
- [Report 5: Multiturn conversation](papers/5/report.pdf) ($n=3$ rounds)
- [Report 6: Multiturn conversation (full transcript) #2](papers/7/report.pdf) ($n=3$ rounds) Highly detailed full transcript version; very interesting
- [Report 7: Multiturn conversation #3](papers/6/report.pdf) ($n=3$ rounds)
- [Report 8: Strong attacker prompt - multiturn](papers/8/report.pdf) ($n=3$ rounds) Highly interesting because of high effectiveness variance for attacker/defender
- [Report 9: Medium attacker prompt - multiturn](papers/9/report.pdf) ($n=3$ rounds)
- [Report 10: Strong attacker prompt - multiturn (20 turns)](papers/10/report.pdf) ($n=3$ rounds)
- [Report 11: Medium attacker prompt - multiturn](papers/11/report.pdf) ($n=5$ rounds)
